{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRVmUk7DPenG"
      },
      "source": [
        "# 단어 및 글자 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihrc1_TIG4bQ",
        "outputId": "6f98ec23-cbde-4ec0-8814-3d3015272ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['현실과', '구분', '불가능한', 'cg', '시각적', '즐거움은', '최고!', '더불어', 'ost느', '더더욱', '최고!!']\n"
          ]
        }
      ],
      "source": [
        "#단어 토큰화\n",
        "review=\"현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost느 더더욱 최고!!\"\n",
        "tokenized = review.split()\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwPzLim8PN0x",
        "outputId": "2d6ad6b6-860f-44a7-b5ac-0bedb3fcc913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '느', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "#글자 토큰화\n",
        "tokenized = list(review)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvPfodsmSP7J",
        "outputId": "3b0286f1-8207-454b-a999-e2ef3dfc0953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jamo in /usr/local/lib/python3.10/dist-packages (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install jamo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "c2lSIZU-T7Ot",
        "outputId": "d3ec0da1-793f-433e-fa18-fbe2605b71ca"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'jamo' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a082566cab19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m retval =jamo.h2j(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhangul_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jamo' is not defined"
          ]
        }
      ],
      "source": [
        "#자모 변환 함수\n",
        "retval =jamo.h2j(\n",
        "    hangul_string\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "-QpcqBWWUAiQ",
        "outputId": "704669a9-1fb9-40ca-aa5f-a8938f7e4534"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'jamo' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-15a968527a13>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#한글 호환성 자모 변환 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m retval = jamo.jhcj(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mjamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jamo' is not defined"
          ]
        }
      ],
      "source": [
        "#한글 호환성 자모 변환 함수\n",
        "retval = jamo.jhcj(\n",
        "    jamo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFc_KGV0SFyv",
        "outputId": "0e545d2b-9547-4495-e5fc-c7601be49236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅡ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "#자소 단위 토큰화\n",
        "from jamo import h2j, j2hcj\n",
        "\n",
        "\n",
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "decomposed = j2hcj(h2j(review)) #h2j는 자모 변환 함수, j2hcj는 한글 호환 자모 변환 함수로 조합형 한글 문자열을 자소 단위로 나눠 반환함.\n",
        "tokenized = list(decomposed)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L-ZTR7pO3m9"
      },
      "source": [
        "# 형태소 토큰화\n",
        "KoNLPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdvct9nP0yY",
        "outputId": "f4e23e49-319d-4433-88aa-d33b6d232372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27x55Tn8PCZd",
        "outputId": "3ee5c258-478c-4a4c-df82-97cf2226ca1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
            "구 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
            "형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
            "품사 태깅 : [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ],
      "source": [
        "#Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = okt.nouns(sentence)\n",
        "phrases = okt.phrases(sentence)\n",
        "morphs = okt.morphs(sentence)\n",
        "pos = okt.pos(sentence)\n",
        "\n",
        "print(\"명사 추출 :\", nouns)\n",
        "print(\"구 추출 :\", phrases)\n",
        "print(\"형태소 추출 :\", morphs)\n",
        "print(\"품사 태깅 :\", pos) #(형태소,품사) 형태의 튜플로 구성된 리스트를 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUym2Q2-PANE",
        "outputId": "85070e29-1603-4f7b-b8da-650c432422e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "명사 추출 : ['무엇', '상상', '수', '사람', '무엇']\n",
            "문장 추출 : ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']\n",
            "형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
            "품사 태깅 : [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ],
      "source": [
        "#꼬꼬마 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = kkma.nouns(sentence)\n",
        "sentences = kkma.sentences(sentence)\n",
        "morphs = kkma.morphs(sentence)\n",
        "pos = kkma.pos(sentence)\n",
        "\n",
        "print(\"명사 추출 :\", nouns)\n",
        "print(\"문장 추출 :\", sentences)\n",
        "print(\"형태소 추출 :\", morphs) #okt 분석기와 다르게 출력된다.\n",
        "print(\"품사 태깅 :\", pos) #태깅하는 품사의 수가 많으면 더 자세한 단위로 분석 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbxfBudBTIEn",
        "outputId": "4ee67e4c-cd20-492e-d8f9-a2bb34fbde2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#NLTK 패키지 및 모델 다운로드\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnwHMbFXWaYc",
        "outputId": "0f8c3713-2897-4158-a911-814a9f1da1be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
            "['Those who can imagine anything, can create the impossible.']\n"
          ]
        }
      ],
      "source": [
        "#영문 토큰화\n",
        "from nltk import tokenize\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "sent_tokens = tokenize.sent_tokenize(sentence)\n",
        "\n",
        "print(word_tokens)\n",
        "print(sent_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-uEKsb5Wx1o",
        "outputId": "ef1089dd-384d-4f70-bdba-88e42a31ad74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "#영문 품사 태깅\n",
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "pos = tag.pos_tag(word_tokens)  #품사 태깅\n",
        "\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCqN097QfXxZ",
        "outputId": "1169bcb8-5afc-4739-dd13-8f33d8bffd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PRON  - DT ] : Those\n",
            "[PRON  - WP ] : who\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : imagine\n",
            "[PRON  - NN ] : anything\n",
            "[PUNCT - ,  ] : ,\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : create\n",
            "[DET   - DT ] : the\n",
            "[ADJ   - JJ ] : impossible\n"
          ]
        }
      ],
      "source": [
        "#spaCy 품사 태깅\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\") #모델 불러오기\n",
        "sentence = \"Those who can imagine anything, can create the impossible\"\n",
        "doc = nlp(sentence) #객체 지향적으로 구현돼 처리한 결과를 doc 객체에 저장한다.\n",
        "\n",
        "for  token in doc:\n",
        "   print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRujq942p7t"
      },
      "source": [
        "# 바이트 페어 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5bU1i0vZ2o-1",
        "outputId": "4f3f7f68-f649-48ba-b9fc-07bc7d276f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.8.30)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, Korpora\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "6983b311c4fc455ba352171ffb0f238d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install sentencepiece Korpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ67X3PTf2F-",
        "outputId": "61a509ba-76cd-4ea1-98c4-cc83449758c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 7.15MB/s]                            \n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 114MB/s]                             \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 15.9MB/s]                            \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 159MB/s]                             \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 31.8MB/s]                            \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:01, 23.9MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 157MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 163MB/s]                            \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 138MB/s]                            \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 190MB/s]                             \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 166MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 178MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:01, 24.3MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 163MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 182MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:01, 23.2MB/s]                            \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 183MB/s]                             \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:01, 27.3MB/s]                            \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 168MB/s]                             \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:01, 22.6MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청원 시작일 : 2017-08-25\n",
            "청원 종료일 : 2017-09-24\n",
            "청원 동의 수 : 88\n",
            "청원 범주 : 육아/교육\n",
            "청원 제목 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n",
            "청원 본문 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"
          ]
        }
      ],
      "source": [
        "# 청와대 청원 데이터 다운로드\n",
        "\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\") #말뭉치 불러오기를 통해 말뭉치 데이터 다운로드한다.\n",
        "dataset = corpus.train\n",
        "petition = dataset[0]\n",
        "\n",
        "print(\"청원 시작일 :\", petition.begin)\n",
        "print(\"청원 종료일 :\", petition.end)\n",
        "print(\"청원 동의 수 :\", petition.num_agree)\n",
        "print(\"청원 범주 :\", petition.category)\n",
        "print(\"청원 제목 :\", petition.title)\n",
        "print(\"청원 본문 :\", petition.text[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp2XGv_x-mQ9",
        "outputId": "5606b33c-caf8-4629-87f8-9a460b3065f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kcbert': 'beomi@github 님이 만드신 KcBERT 학습데이터',\n",
              " 'korean_chatbot_data': 'songys@github 님이 만드신 챗봇 문답 데이터',\n",
              " 'korean_hate_speech': '{inmoonlight,warnikchow,beomi}@github 님이 만드신 혐오댓글데이터',\n",
              " 'korean_parallel_koen_news': 'jungyeul@github 님이 만드신 병렬 말뭉치',\n",
              " 'korean_petitions': 'lovit@github 님이 만드신 2017.08 ~ 2019.03 청와대 청원데이터',\n",
              " 'kornli': 'KakaoBrain 에서 제공하는 Natural Language Inference (NLI) 데이터',\n",
              " 'korsts': 'KakaoBrain 에서 제공하는 Semantic Textual Similarity (STS) 데이터',\n",
              " 'kowikitext': 'lovit@github 님이 만드신 wikitext 형식의 한국어 위키피디아 데이터',\n",
              " 'namuwikitext': 'lovit@github 님이 만드신 wikitext 형식의 나무위키 데이터',\n",
              " 'naver_changwon_ner': '네이버 + 창원대 NER shared task data',\n",
              " 'nsmc': 'e9t@github 님이 만드신 Naver sentiment movie corpus v1.0',\n",
              " 'question_pair': 'songys@github 님이 만드신 질문쌍(Paired Question v.2)',\n",
              " 'modu_news': '국립국어원에서 만든 모두의 말뭉치: 뉴스 말뭉치',\n",
              " 'modu_messenger': '국립국어원에서 만든 모두의 말뭉치: 메신저 말뭉치',\n",
              " 'modu_mp': '국립국어원에서 만든 모두의 말뭉치: 형태 분석 말뭉치',\n",
              " 'modu_ne': '국립국어원에서 만든 모두의 말뭉치: 개체명 분석 말뭉치',\n",
              " 'modu_spoken': '국립국어원에서 만든 모두의 말뭉치: 구어 말뭉치',\n",
              " 'modu_web': '국립국어원에서 만든 모두의 말뭉치: 웹 말뭉치',\n",
              " 'modu_written': '국립국어원에서 만든 모두의 말뭉치: 문어 말뭉치',\n",
              " 'open_subtitles': 'Open parallel corpus (OPUS) 에서 제공하는 영화 자막 번역 병렬 말뭉치',\n",
              " 'aihub_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어 + 대화 + 뉴스 + 한국문화 + 조례 + 지자체웹사이트)',\n",
              " 'aihub_spoken_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어)',\n",
              " 'aihub_conversation_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (대화)',\n",
              " 'aihub_news_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (뉴스)',\n",
              " 'aihub_korean_culture_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (한국문화)',\n",
              " 'aihub_decree_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (조례)',\n",
              " 'aihub_government_website_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (지자체웹사이트)'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from Korpora import Korpora\n",
        "Korpora.corpus_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLx9hdnJ3M4P",
        "outputId": "4c72190c-b2dd-45d4-cad4-a4cb423544d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 20.0MB/s]\n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 119MB/s]                             \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 88.8MB/s]                            \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 157MB/s]                             \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 152MB/s]                             \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 155MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 147MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 151MB/s]                             \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 145MB/s]                            \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:01, 23.3MB/s]                            \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 121MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:01, 27.2MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 151MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 154MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 176MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 167MB/s]                            \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 151MB/s]                             \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 164MB/s]                             \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:01, 24.5MB/s]                            \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 153MB/s]                            \n"
          ]
        }
      ],
      "source": [
        "#학습 데이터세트 생성\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\", force_download=True)\n",
        "petitions = corpus.get_all_texts() #본문 데이터세트를 한 번에 불러올 수 있다.\n",
        "with open('/content/drive/MyDrive/datasets/corpus.txt', 'w', encoding='utf-8') as f:\n",
        "  for petition in petitions:\n",
        "    f.write(petition+'\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tLfXgNR_4jcW"
      },
      "outputs": [],
      "source": [
        "#토크나이저 모델 학습\n",
        "# 말뭉치 텍스트 파일경로, 모델 파일 이름, 어휘 사전 크기를 설정한다.\n",
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "SentencePieceTrainer.Train(\n",
        "    \"--input=/content/drive/MyDrive/datasets/corpus.txt\\\n",
        "    --model_prefix=petition_bpe\\\n",
        "    --vocab_size=8000 model_type=bpe\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oUvhA_-4_9Z",
        "outputId": "6fc0001e-04ba-499e-826d-43b994ab15ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '되었', '군요', '!']\n",
            "여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]\n",
            "단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]\n",
            "여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n",
            "정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n",
            "하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"
          ]
        }
      ],
      "source": [
        "#바이트 페어 인코딩 토큰화\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"petition_bpe.model\")\n",
        "\n",
        "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
        "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
        "\n",
        "tokenized_sentence = tokenizer.encode_as_pieces(sentence) #문장을 토큰화한다.\n",
        "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
        "print(\"단일 문장 토큰화 :\", tokenized_sentence)\n",
        "print(\"여러 문장 토큰화 :\", tokenized_sentences)\n",
        "\n",
        "encoded_sentence = tokenizer.encode_as_ids(sentence)   #토큰을 정수로 인코딩해 제공\n",
        "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
        "print(\"단일 문장 정수 인코딩 :\", encoded_sentence)\n",
        "print(\"여러 문장 정수 인코딩 :\", encoded_sentences)\n",
        "\n",
        "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
        "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
        "print(\"정수 인코딩에서 문장 변환 :\", decode_ids) #문자열 데이터로 변환한다.\n",
        "print(\"하위 단어 토큰에서 문장 변환 :\", decode_pieces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWA8f2Sq5_8L",
        "outputId": "3e765eeb-6a4c-4693-aa98-fa766db287dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]\n",
            "vocab size : 8000\n"
          ]
        }
      ],
      "source": [
        "#어휘 사전 불러오기\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"petition_bpe.model\")\n",
        "\n",
        "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
        "#id_o_piece 메서드를 통해 정숫값을 하위 단어로 변환하는 메서드, 하위 단어 개수만큼 반복해 하위단어의 개수를 반환\n",
        "print(list(vocab.items())[:5])\n",
        "print(\"vocab size :\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhPB_88v5vmq"
      },
      "outputs": [],
      "source": [
        "#워드피스 토크나이저 학습\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece())\n",
        "tokenizer.normalizer = Sequence([NFD(), Lowercase()]) #정규화 클래스 중 NFD 유니코드 정규화와 대문자를 소문자로 변환하는 Lowercase() 사용\n",
        "tokenizer.pre_tokenizer = Whitespace() #사전 토큰화 클래스 중 단어 경계 기준으로 설정한다.\n",
        "\n",
        "tokenizer.train([\"/content/drive/MyDrive/datasets/corpus.txt\"]) #훈련메서드\n",
        "tokenizer.save(\"/content/drive/MyDrive/datasets/models/petition_wordpiece.json\") #학습결과 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z31rHHLQ6zmE"
      },
      "outputs": [],
      "source": [
        "#워드피스 토큰화\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/datasets/models/petition_wordpiece.json\")\n",
        "tokenizer.decoder = WordPieceDecoder() #디코더를 워드피스 디코더로 설정한다\n",
        "\n",
        "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
        "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
        "\n",
        "encoded_sentence = tokenizer.encode(sentence)\n",
        "encoded_sentences = tokenizer.encode_batch(sentences) #batch 메서드를 통해 여러 문장을 한번에 토큰화\n",
        "\n",
        "print(\"인코더 형식 :\", type(encoded_sentence))\n",
        "\n",
        "print(\"단일 문장 토큰화 :\", encoded_sentence.tokens)\n",
        "print(\"여러 문장 토큰화 :\", [enc.tokens for enc in encoded_sentences])\n",
        "\n",
        "print(\"단일 문장 정수 인코딩 :\", encoded_sentence.ids)\n",
        "print(\"여러 문장 정수 인코딩 :\", [enc.ids for enc in encoded_sentences])\n",
        "\n",
        "print(\"정수 인코딩에서 문장 변환 :\", tokenizer.decode(encoded_sentence.ids)) #정수를 다시 문장으로 변환하는 경우 디코딩 메서드를 통해 정수 인코딩된 결과를 다시 문장으로 디코딩해 출력\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}