{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram"
      ],
      "metadata": {
        "id": "z6_u62kxmeBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfeHcC1Bmcoe",
        "outputId": "fc62bf19-8fe1-482e-ea91-fb453096c172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def ngrams(sentence, n): # n은 n-그램의 길이\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    # 단어 리스트를 순회하면서, 시작점을 다르게 하는 여러 리스트를 생성\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n",
        "\n",
        "unigram = ngrams(sentence, 1)\n",
        "bigram = ngrams(sentence, 2)\n",
        "trigram = ngrams(sentence, 3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "#파이썬 코드 N-gram과 NLTK라이브러리에서 지원하는 N-gram 함수는 동일한 출력값을 반환한다.\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "sOzc5ESEk7ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip instal scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISy9-d-vmt3e",
        "outputId": "1fe831b5-80ab-4765-a4df-952741f62ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 클래스\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    input=\"content\", #문자열 데이터 혹은 바이트 형태로 입력값\n",
        "    encoding = \"utf-8\", #텍스트 인코딩 값\n",
        "    lowercase=True,#소문자 변환여부\n",
        "    stop_words=None, #불용어로 분석에 도움이 되지 않ㅇ는 의미 없는 단어들은 추가되지 않음\n",
        "    ngram_range=(1,1), #n-gram의 범위로 (최솟값, 최댓값)형태\n",
        "    max_df=1.0, #최댓값 문서 빈도로 1이하의 실수를 입력하면 해당 비율을 초과해 등장한 단어를 불용어로 처리\n",
        "    min_df=1, #최솟값 문서 빈도\n",
        "    vocabulary=None, #단어 사전을 의미하며 입력하지 않으면 자동으로 구축\n",
        "    smooth_idf=True, #IDF 분모처리로 idf 계산시 분모에 1을 더한다.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "uHXPM3dWk9bN",
        "outputId": "f81d1ea9-c614-4d5a-f5f0-1b09748fd96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sklearn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-31a3ec821320>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TF-IDF 클래스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#문자열 데이터 혹은 바이트 형태로 입력값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#텍스트 인코딩 값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#소문자 변환여부\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 계산\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I don’t like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray()) #toarray 메서드는 TF-IDF를 넘파이 배열로 변환\n",
        "# (문서수 X 단어 수)의 형태로 가진다.\n",
        "print(tfidf_vectorizer.vocabulary_) #TF-IDF에 사용된 단어 사전을 의미\n",
        "# 해당 단어에 대한 색인 값을 의미함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "othN8GpvmZ3V",
        "outputId": "ee84e4d4-752c-4263-c291-47710b2674f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram 모델 실습"
      ],
      "metadata": {
        "id": "enSSGrpq5zmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 클래스\n",
        "embedding = torch.nn.Embedding(\n",
        "    num_embeddings, #임베딩의 수\n",
        "    embedding_dim, #임베딩 베터의 크기\n",
        "    padding_idx=None, #입력 문장들을 일정한 길이로 맞추는 역할\n",
        "    max_norm=None, #임베딩 베터의 최대 크기\n",
        "    norm_type=2.0 #노름 타입 기본값이 2면 L2 정규화방식 사용\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "AsZVjxLB40Od",
        "outputId": "1a43570f-f0bf-4627-c1e5-c2c10e2c6a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b325df2bb5c8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#임베딩 클래스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m embedding = torch.nn.Embedding(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#임베딩의 수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#임베딩 베터의 크기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#입력 문장들을 일정한 길이로 맞추는 역할\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기본 Skip-gram 클래스\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size, #임베딩의 수(단어 사전의 크기)\n",
        "            embedding_dim=embedding_dim #임베딩 베터의 크기\n",
        "        )\n",
        "        self.linear = nn.Linear( #선형변환 클래스\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "PvwF2ZSQ6epS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADywSpQ98m90",
        "outputId": "00943ae2-6814-4ccd-d709-c88aad2f5c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (24.1)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Package 'python-dev' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCmXaj-o8-rt",
        "outputId": "5a7d2f6c-1647-4364-e19e-e4f005bbe092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTepkjpA8udW",
        "outputId": "c29e148a-66b4-4704-d338-b4e4f8ecfd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영화리뷰 데이터세트 전처리\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "#영화리뷰 감정 분석 데이터세트 불러옴\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)\n",
        "\n",
        "#OKT 토크나이저를 사용해 형태소를 추출함.\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InpgTxGp7Kt1",
        "outputId": "ac10f002-2c97-4fc5-d54e-22e3c3e87dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 사전 구축\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens): #build_vocab 함수로 단어 사전 구축\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        " #n_vocab 매개변수는 구축할 단어 사전의 크기\n",
        " #special_tokens는 현재 1개임.\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)} #각 토큰을 해당하는 인덱스(ID)로 매핑\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)} #각 인덱스(ID)를 해당하는 토큰으로 매핑\n",
        "\n",
        "print(vocab[:10]) #10개 단어 추출\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-30ltZM9rhI",
        "outputId": "fa491ec9-b21a-4789-ce1e-afe37859193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#skip-gram의 단어 쌍 추출\n",
        "def get_word_pairs(tokens, window_size): #window_size는 주변 단어를 몇 개까지 고려할 것인지\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence): #idx는 현재 단어의 인덱스, center_word는 중심 단어\n",
        "            #현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지 ㄱㄹ정\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]  #idx는 현재 단어의 인덱스, center_word는 중심 단어\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ2OQrVG9yOj",
        "outputId": "49db24ec-6593-4b3e-91ca-fa78813eb0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인덱스 쌍 변환\n",
        "def get_index_pairs(word_pairs, token_to_id): #생성된 단어쌍을 토큰 인덱스 쌍으로 변환\n",
        "# 앞에서 생성한 word_pairs와 ID를 매핑한 디셔너리인 token_to_id로 인덱스 쌍을 생성한다.\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"] #단어 사전 내에 없으면 unk토큰 인덱스 반환\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2n0snLQAsVM",
        "outputId": "8f9064b2-c426-4d09-f634-116099f92bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KzuBkGUB5z6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 설정\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "index_pairs = torch.tensor(index_pairs) #get_index_pairs 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루는 리스트\n",
        "# 텐서 형식으로 변환\n",
        "center_indexs = index_pairs[:, 0]\n",
        "contenxt_indexs = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexs, contenxt_indexs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "wLvNBCo6Cxiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip-gram 모델 준비 작업\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "# 전체 단어 집합의 크기를 전달, 임베딩 크기는 128로 할당\n",
        "criterion = nn.CrossEntropyLoss().to(device) #교차 엔트로피 사용\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "PqD31DQMDQ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        #가중치와 편향을 갱신하기\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "id": "O2usYvElZco_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 값 추출\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[30]\n",
        "token_embedding = token_to_embedding[token] #임베딩 값 추출하기\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pgfPZdcQZ6pj",
        "outputId": "4e37f8e5-ef3f-44bb-90a3-e2b5e94fba85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word2vec' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-62a114f901f4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#임베딩 값 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtoken_to_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 임베딩 유사도 계산\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a)) #a는 임베딩 토큰, b는 임베딩 행렬을 의미\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n): #유사도 행렬을 내림차순으로 정렬해 어떤 단어가 가장 가까운 단어인지 반환한다 .\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "id": "DO1jrTDabP9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim 모델 실습"
      ],
      "metadata": {
        "id": "gs4DWd8_lC92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QhK8ZURWdvu",
        "outputId": "cd429832-d047-4b14-8f54-cc7b9691a856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njL1NXNZZ3qQ",
        "outputId": "0e9748f4-b936-4d47-c7c1-1a4986262a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1No2rehaHfX",
        "outputId": "9850de0e-a4a9-4eb0-f80e-effd18157707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,377 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,071 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,424 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,601 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Fetched 19.0 MB in 8s (2,379 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "Collecting JPype1\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (24.1)\n",
            "Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 488.6/488.6 kB 6.0 MB/s eta 0:00:00\n",
            "Installing collected packages: JPype1\n",
            "Successfully installed JPype1-1.5.0\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/19.4 MB 22.3 MB/s eta 0:00:00\n",
            "Installing collected packages: konlpy\n",
            "Successfully installed konlpy-0.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Package 'python-dev' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Korpora 사용하기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZWn4auYZurA",
        "outputId": "bec82b94-ecd1-4f97-df5f-4c7dcbe715e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 49.9MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 26.0MB/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#okt토크나이저 사용하기\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "zEEO9I7LZ936"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델학습\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens, #입력문장으로 토큰 리스트로 표현된다.\n",
        "    vector_size=128, #벡터의 크기\n",
        "    window=5, #윈독가 5이니 5만큼 떨어진 단어까지 주변 단어로 고려해서 보겠다\n",
        "    min_count=1, #최소빈도는 고려하지 않는다.\n",
        "    sg=1,#skip-gram을 사용한다.\n",
        "    epochs=3, #학습 에폭의 수를 3으로 두겠다\n",
        "    max_final_vocab=10000 #최대 단어의 사전 크기\n",
        ")"
      ],
      "metadata": {
        "id": "HWReaMuoaJ0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 추출 및 유사도 계산하기\n",
        "ord = \"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5)) #주어진 단어에 대해 가장 유사한 단어를 찾는다.\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\")) #두 단어 간의 유사도를 계산한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S99ff9F-bB65",
        "outputId": "29dca9d0-10af-422b-9e0f-05995dfd7b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.95674318e-01 -4.17733222e-01  1.23469524e-01  1.32955298e-01\n",
            " -7.24665895e-02 -8.72441083e-02  9.83308032e-02 -1.77862570e-01\n",
            " -4.54244286e-01  5.03616095e-01 -1.23775157e-04 -3.60341966e-01\n",
            " -2.27396905e-01  1.19375162e-01  1.44563824e-01 -1.37856796e-01\n",
            " -3.08990687e-01  3.66759390e-01 -1.02352515e-01  2.13525087e-01\n",
            "  6.14885807e-01  1.69751495e-01 -1.00055300e-01  4.27087024e-02\n",
            " -3.20105880e-01 -1.79719999e-02 -4.65251595e-01  1.54421777e-01\n",
            "  1.71137124e-01 -1.58841237e-01 -5.31283617e-01  7.93731064e-02\n",
            "  3.75958413e-01 -1.99818999e-01 -1.07294574e-01 -5.36741167e-02\n",
            " -8.08436498e-02 -2.16511652e-01 -4.66581881e-02 -2.79120982e-01\n",
            " -1.52311057e-01 -1.87538892e-01 -3.28609109e-01 -5.19733250e-01\n",
            " -2.68020034e-01  1.27620921e-01 -3.77623022e-01  3.62898931e-02\n",
            "  3.42305064e-01  3.82988267e-02  3.31874400e-01  3.39366823e-01\n",
            "  2.73992926e-01  1.91310093e-01 -3.70166361e-01 -2.97885507e-01\n",
            "  9.96721014e-02  3.84469450e-01 -1.77470863e-01  1.06556110e-01\n",
            "  1.08735338e-01 -2.12108225e-01  3.02026331e-01  2.74776995e-01\n",
            " -2.37614751e-01  5.42985201e-02 -5.57606034e-02  1.55899912e-01\n",
            "  2.04038009e-01 -2.73554891e-01 -3.85170221e-01 -1.74502611e-01\n",
            " -5.81524074e-01  1.07606158e-01  3.14024985e-02 -6.43378496e-02\n",
            " -1.22613227e-02 -2.79701233e-01 -2.76397634e-02  2.44238358e-02\n",
            " -3.09825033e-01 -6.29694685e-02  2.74465472e-01  8.12078774e-01\n",
            "  3.49373609e-01  1.25552595e-01  2.54308760e-01 -5.33195674e-01\n",
            "  1.35534659e-01 -2.44343236e-01 -3.19081962e-01  5.33871725e-02\n",
            " -1.64994299e-02  4.60439444e-01 -3.59623097e-02  3.12093627e-02\n",
            " -3.85824531e-01 -2.20981851e-01 -2.26387650e-01 -5.99696755e-01\n",
            " -2.91571528e-01  3.81461792e-02  2.23586276e-01 -3.32127512e-01\n",
            " -3.83409089e-03  2.86461502e-01  3.19029063e-01  1.93067044e-01\n",
            "  8.02515745e-02 -2.60222912e-01  3.69327307e-01 -4.05944616e-01\n",
            " -1.60765141e-01  2.76431054e-01  1.54891014e-01 -5.20889834e-02\n",
            "  4.24643934e-01  2.51858056e-01 -1.30502582e-01  3.60107809e-01\n",
            " -3.20824265e-01 -3.57428133e-01  9.61688310e-02  7.48547679e-03\n",
            "  1.53589487e-01  9.43217054e-02 -5.30158937e-01 -1.24053143e-01]\n",
            "[('연기력', 0.7788348197937012), ('몰입도', 0.7275864481925964), ('연기자', 0.7265974283218384), ('몸매', 0.7259952425956726), ('캐스팅', 0.7213491797447205)]\n",
            "0.7788348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastText 모델 실습"
      ],
      "metadata": {
        "id": "MUUMSHV5opdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#KorNLI 데이터세트 전처리\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apKmspKLbvjs",
        "outputId": "5176b28d-82d2-4cb4-d7e0-233273adf2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
            "    References :\n",
            "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
            "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
            "           (https://arxiv.org/abs/2004.03289)\n",
            "\n",
            "    This is the dataset repository for our paper\n",
            "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
            "    (https://arxiv.org/abs/2004.03289)\n",
            "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
            "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[kornli] download multinli.train.ko.tsv: 83.6MB [00:00, 170MB/s]                            \n",
            "[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 99.6MB/s]                           \n",
            "[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 4.66MB/s]                           \n",
            "[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 8.19MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 실습\n",
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    min_n=2, #n-gram 범위를 [2,6]으로 설정했다.\n",
        "    max_n=6\n",
        ")\n",
        "\n",
        "# fastText.save(\"../models/fastText.model\")\n",
        "# fastText = FastText.load(\"../models/fastText.model\")"
      ],
      "metadata": {
        "id": "8U0QozgYqkXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = \"사랑해요\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key) #학습된 단어 사전을 나타내는 리스트인데 oov니까 false로 출력됨.\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNUyDEZ8qzYD",
        "outputId": "a7d49d8b-dd43-4057-a6a4-257def87913e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "[('사랑해', 0.9124645590782166), ('사랑', 0.8615164160728455), ('사랑한', 0.8545567393302917), ('사랑해.', 0.8494171500205994), ('사랑해서', 0.8457275629043579)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망"
      ],
      "metadata": {
        "id": "YG5W04mvlB4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 신경망\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "\n",
        "model = nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    nonlinearity=\"tanh\",\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size) #입력값 차원은 [배치크기, 시퀀스 크기, 입력 특성 크기]\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size) #은닉 상태는 [계층수*양방향 여부+1, 배치크기, 은닉 상태크기]\n",
        "\n",
        "outputs, hidden = model(inputs, h_0) #입력값과 초기 은닉 상태로 순방향 연산을 수행해 출력값과 최종 은닉 상태를 반환\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "id": "T4Q9Rm4_saKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c419a654-81c5-4b23-f405-c6dac4bcb7c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 512])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 장기 메모리"
      ],
      "metadata": {
        "id": "nQDryKyQCBZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 장단기 메모리\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "proj_size = 64\n",
        "\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size) #입력값\n",
        "h_0 = torch.rand( #초기 은닉 상태\n",
        "    num_layers * (int(bidirectional) + 1),\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else ouput_size,\n",
        ")\n",
        "#초기 메모리 셀 상태\n",
        "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
        "\n",
        "print(outputs.shape) #출력값\n",
        "print(h_n.shape) #최종 은닉 상태\n",
        "print(c_n.shape) #최종 메모리 셀 상태"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9TsMbK29rQM",
        "outputId": "0ba21195-b120-4978-ec27-b11708a225b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 128])\n",
            "torch.Size([6, 4, 64])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1123: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ../aten/src/ATen/native/RNN.cpp:1475.)\n",
            "  result = _VF.lstm(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망/장단기 신경망 모델실습 1) 무작위 값으로 할당된 임베딩 계층으로 모델 구성"
      ],
      "metadata": {
        "id": "1PQDVN9hCsxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 분류 모델\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab, #단어 사전 크기\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\" #모델 종류는 장단기 메모리 사용\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        if model_type == \"rnn\": #순환신경망\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\": #장기메모리라면?\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GgVOIcZlCxBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터세트 불러오기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv71n238ES6R",
        "outputId": "b8bd8269-2334-4554-e176-3c2a264360ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = corpus_df.sample(frac=0.9, random_state=42) #테스트데이터 불러오기\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_ardv9_EWpw",
        "outputId": "cd5830cf-8429-484f-c098-f7fd7e080df9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data Size : 45000\n",
            "Testing Data Size : 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 토큰화 및 단어 사전 구축\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens): #문장의 길이를 맞추기 위해 스페셜토큰 추가\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwZWWWEzEpu4",
        "outputId": "009f29c1-d34b-45ee-bec5-a8b1d8809c8d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "5002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#정수 인코딩 및 패딩\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32 #입력 텍스트의 길이를 고려해 결정한다.\n",
        "pad_id = token_to_id[\"<pad>\"] #너무 긴 문장은 최대길이로 줄이기\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id) #시퀀스를 최대 길이로 잘라내게함.\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgS79YXUEsga",
        "outputId": "8fbd83de-1163-4967-fe7e-0e66878e2606"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels) #정수 인코딩과 라벨값을 파이토치 텐서 형태로 변환한다.\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "3ifO5ccuFaJT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64 #은닉상태 크기\n",
        "embedding_dim = 128 #임베딩 벡터의 크기\n",
        "n_layers = 2 # -> 신경망은 두개의 층으로 구성된다!!\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device) #손실함수는 이진 교차엔트로피 함수 적용\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001) #최적화 함수는 지수 가중 이동 평균을 사용한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "5IHmylMxFxsW",
        "outputId": "a6ed5a96-2cdc-42a4-de9f-02cd8c634100"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SentenceClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4b338dafe0bd>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m classifier = SentenceClassifier(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mn_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ).to(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentenceClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 및 테스트\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list() #검증 손실\n",
        "    corrects = list() #검증 정확도\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "    # 검증손실과 검증 정확도를 확인한다.\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs): #에폭마다 테스트 데이터 세트로\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "flHqsS-mHtb7",
        "outputId": "e002cfbd-66db-4693-c54a-ae2799d01ea8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b3f5cb6e728e>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#에폭마다 테스트 데이터 세트로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델로부터 임베딩 추출\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = emb\n",
        "\n",
        "token = vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "2pzGDgHfJTkk",
        "outputId": "affd559f-26fc-4a5d-b287-15d2c913e957"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-efa9d37bd195>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습된 모델로부터 임베딩 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtoken_to_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 사전에 학습된 모델로 학습"
      ],
      "metadata": {
        "id": "pd04WcVCJ5W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 계층 초기화\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]: 두 개의 토큰은 초기화에서 제외한다.\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "dpSYxSP-J39j",
        "outputId": "9b1281bf-3561-4855-8551-eddbdfccca5d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../models/word2vec.model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6f60f95611ff>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/word2vec.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0minit_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \"\"\"\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/word2vec.model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#사전 학습된 임베딩 계층 적용\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\",\n",
        "        pretrained_embedding=None #사전 학습된 임베딩 매개변수가 None이 아니라면 전달된 값을 임베딩 계층으로 초기화한다.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0\n",
        "            )\n",
        "\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PQ5pvr7-Kepe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "cZIBzJkfLE69",
        "outputId": "c67a30d3-a6a9-4174-9a49-f0396aa2bb74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SentenceClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b2349640692a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m classifier = SentenceClassifier(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mn_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentenceClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱"
      ],
      "metadata": {
        "id": "qNspjhoJsGNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 모델\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential( #선형 변환으로 구성\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
        "            ),\n",
        "            nn.ReLU(), #렐루 함수 사용\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(32 * 32 * 32, 10) #완전 연결 계층의 입력 데이터 차원의 크기는\n",
        "        # 마지막 특징 맵의 높이 X 너비 X 채널로 계산된다.\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fVK2eroXsLqd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 기반 문장 분류 모델 정의\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim = self.embedding.weight.shape[1]\n",
        "\n",
        "        conv = []\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d( #1차원 합성곱 계층\n",
        "                        in_channels=embedding_dim,\n",
        "                        out_channels=1,  #출력 채널 수 1\n",
        "                        kernel_size=size\n",
        "                    ),\n",
        "                    nn.ReLU(), #렐루함수 사용\n",
        "                    nn.MaxPool1d(kernel_size=max_length-size-1), #최댓값 플링 구성 ': 합성곱 연산의 영향을 받았으므로\n",
        "                    #최대길이 - 필터크기 - 1로 설정\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters = nn.ModuleList(conv) #여러 개의 서브모듈을 리스트 형태로 저장하는 기능 제공\n",
        "\n",
        "        output_size = len(filter_sizes)\n",
        "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = embeddings.permute(0, 2, 1) #permute 메서드를 이용해 차원을 2에서 1로 변경하고 여러 개의 합성곱 필터에 통과시킴\n",
        "\n",
        "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
        "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        logits = self.pre_classifier(concat_outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.classifier(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ExjkWwN6zhIy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc7eDr735x3L",
        "outputId": "c3f10ad1-442c-43ac-c1bb-53c8a4a4537f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CETjWs035ODE",
        "outputId": "3cf540e9-3bbd-442f-cb1b-f03863892304"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 113MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 79.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyy6vted5p1k",
        "outputId": "bc8df029-2a21-4221-e666-489e73ce0573"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data Size : 45000\n",
            "Testing Data Size : 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9apOLAt6XMy",
        "outputId": "a3faca04-7bc7-4a49-d572-1cce730ed618"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,074 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,605 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,431 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,377 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,208 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,286 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Fetched 25.6 MB in 4s (5,816 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "Collecting JPype1\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (24.1)\n",
            "Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 488.6/488.6 kB 22.1 MB/s eta 0:00:00\n",
            "Installing collected packages: JPype1\n",
            "Successfully installed JPype1-1.5.0\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/19.4 MB 87.1 MB/s eta 0:00:00\n",
            "Installing collected packages: konlpy\n",
            "Successfully installed konlpy-0.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Package 'python-dev' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxz6-OPs5-Qh",
        "outputId": "6c164574-e719-4d79-ff62-670b0c8cbe26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "5002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN6iMkPc6a_n",
        "outputId": "11d5af90-b2d0-4c5d-c9dc-ad9b0378b9b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "CYnQp6vB7i5V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "KZcv49Vo7pbO",
        "outputId": "0c10b4fe-c4df-43f7-8fd7-3edc7f919df4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SentenceClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1b9c53744843>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilter_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m classifier = SentenceClassifier(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpretrained_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentenceClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list()\n",
        "    corrects = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "eqxK8MoX625o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "RkdmwTWA6LWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}